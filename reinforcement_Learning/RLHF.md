# RLHF

[TOC]

## 强化学习基础回顾

![image-20230624000300032](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230624000300032.png)

智能体通过采取行动与环境进行交互，并返回状态和奖励。奖励我们想要优化的目标，状态只是当前时间智能体所处环境的一种表示，智能体使用策略将该状态映射到一个动作。

强化学习的精妙之处在于其学习过程是**开放式**的，所以智能体只通过看到这些奖励信号并学习如何随着时间的推移对获取到的奖励进行优化，而不去关注奖励实际的来源如何。

这样可以使得我们构建一个可以解决复杂问题的智能体，这是为什么我们选用强化学习来解决场景对话这样一个开放式问题的原因，而这也是我们开始讨论RLHF(Reinforcement Learning From Human Feedback)的地方。

## 为什么要使用基于人类反馈的强化学习

![image-20230624004936299](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230624004936299.png)

其中的关键问题是我们想要建模的**损失函数是什么**？即我们在机器学习系统中要**以一种有意义的可持续的方对人类的价值观进行编码**，像解决日常生活中最常见的复杂问题一样。

问题是如何**创建和实现一个损失函数**？一个损失函数可能要对以下方面进行评估：

1. 什么是有趣的？
2. 什么是符合道德标准的？
3. 什么是安全的？

基于人类反馈的强化学习的目标是将一些复杂的数据集**集成到机器学习模型中**以对这些值进行编码，或者**将这些值编码到模型而非函数中**。

我们想做的是直接与大多数人一起**学习这些价值观，而不是直接将其赋予给所有人**，并在某种程度上对一些价值观进行**主观的甚至错误的**标记。

所以基于人类反馈的强化学习的确是一种及时且有效的，能够为模型创建复杂损失函数的方法之一。

## 大纲

![image-20230624005220087](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230624005220087.png)

我们将从三方面对RLHF进行介绍：

- RLHF的起源
- RLHF的概念概述
- RLHF的具体细节、未来方向以及推论

## RLHF的起源

RLHF起源于决策制定阶段，在**深度强化学习**广泛应用之前，其通过简单的**非神经网络方法**即评价智能体的行为来形成策略。研究者会将每一个智能体的动作标记为好或坏，这就像是创建了一个给予奖励或惩罚的模型，并基于这个模型来形成行动策略。

![image-20230624011104347](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230624011104347.png)

在图片左侧的论文提出了一种名为**Tamer**的框架来解决俄罗斯方块的智能体训练问题，该框架中的奖励模型和策略存在一定程度的分离。

当**基于人类反馈的强化学习在深度强化学习中得到普及**时，图片右侧的论文使用了基于人类轨迹反馈的奖励预测器，所以上述框架流程中的环境状态也可以被称为RL框架中的观察结果，被提供给人们对其进行标记，奖励预测器会根据这些反馈学习预期奖励，也就是在每个状态下，各种可能的动作应得到的奖励。当智能体在新的状态下需要选择动作时，它可以使用奖励预测器来预测每个可能动作的预期奖励，并选择预期奖励最高的动作。

### 大预言模型中的RLHF

OpenAI在早期试图使用RLHF训练一个模型来总结文本，这就像很多人在考试中做的阅读理解一样，所以这包含人类的能力在其中

#### 案例

**prompt**：

- 攻读计算机科学的博士学位还是继续工作？特别是在一个人在拿到学位之后不想继续在学术界继续工作的情况。

如果你将其传递到一个**刚刚经过总结训练的语言模型**中，你会得到以下的输出：

- 我想攻读计算机科学的博士学位，但是我担心未来的发展；我目前是全职工作，但是我担心未来的发展

这显然不是人们书写的逻辑和方式，并且该语言模型有时会出现语法错误， 使得阅读起来有困难

所以在此基础上，OpenAI做了一件事即对该promot**写了一段人工注释案例**：

- 我现在在做一份软件相关的工作，正在决定是否去攻读计算机科学的博士学位以提高自己的技能并探索新的兴趣和挑战

我们现在知道了早期的实验在做什么，而现在我们要使用**RLHF来组合**这些以获得更好的、更人性化的输出结果：

- 现在处于工作状态，但是正在考虑是否要攻读计算机科学的博士学位以免陷入没有居留签证的尴尬局面。有谁出于研究的目的攻读博士学位但是在毕业之后不想加入学术界的吗？

现实中有大量这样的例子，所以很明显，使用RLHF在这样的任务上有很好的效果，特别是当你问到了一些比较关键和重要的话题，你可能不想获得没有用处的或者错误的信息

### ChatGPT

OpenAI告诉我们可以在大模型中使用RLHF，但是我们并不清楚其中的细节，显然在这件事情上OpenAI没有那么的open。

但是有一些关于ChatGPT中使用RLHF的猜测：

- OpenAI在人工添加注释上面花费了大量的资金
- OpenAI修改了RLHF的训练过程

很明显使用了RLHF之后语言模型的效果有着显著的提升，使用过ChatGPT的人能够明确的感受到

## RLHF中的技术细节

![image-20230626213836185](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626213836185.png)

RLHF 是一项涉及多个模型和不同训练阶段的复杂概念，这里我们按三个步骤分解：

- 预训练一个**语言模型 (LM)** ；
- 聚合问答数据并训练一个**奖励模型 (Reward Model，RM)** ；
- 用**强化学习 (RL)** 方式**微调 LM**。

### 预训练语言模型

![image-20230626214048027](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626214048027.png)

通常语言模型**基于transformer**进行构建，在训练过程中涉及以下内容：

- **无监督的序列预测**
- 从网站上进行**数据爬取**
- 对于模型的尺寸**没有一个最佳的答案**（业界的模型参数涵盖范围从100亿到2800亿）

- 

![image-20230626214230747](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626214230747.png)

```markdown
Language model pretraining : dataset

Dataset:

- Reddit, other forums, news, books
- Optionally include human-written text from predefined prompts

Prompts & Text Dataset -> Train Language Model(Initial Language Model)

				|						^

				|-----------------------|

			  Human Augmented Text(Optional)
```



**人类增强文本**是可选步骤，使用它只是为了**覆盖我们原有的数据集**。人类增强文本中包含**提示**和**原始的文本数据集**，这使得这个数据集看起来像从Reddit、其他论坛或新闻中获取的。这样的组合可以为语言模型提供**更丰富和多样化的输入**，使其更好地**理解和模仿真实世界中的对话和讨论**。

![image-20230626214323985](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626214323985.png)

关于该可选步骤，公司可以雇佣一些**数据标注人员对数据进标注**，即对其中的重要问题或重要的提问进行回答和标注，这些处理后的数据将是真正的**高质量的训练语料**，可以基于此**进一步的训练语言模型**，一些论文将其称之为**监督式微调**

接下来我们来讨论怎么使用特定的数据集来**训练奖励模型引入人类的偏好信息并降低损失**，因为我们刚讨论了怎么**从人类不同的价值观中降低损失**，而**不是使用具体的损失函数**。

### 训练奖励模型

![image-20230626214414790](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626214414790.png)

RM 的训练是 RLHF 区别于旧范式的开端，该模型的目标是**将一段输入文本序列映射到奖励值**，数值上对应人的偏好。我们可以用端到端的方式用 LM 建模，或者用模块化的系统建模 (比如对输出进行排名，再将排名转换为奖励) 。这一奖励数值将对后续无缝接入现有的 RL 算法至关重要。





![image-20230626214536804](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626214536804.png)

RM 的**提示 - 生成对文本**是从**预定义数据集中采样生成**的，并**用初始的 LM 给这些提示生成文本**。即该奖励模型的训练数据集与语言模型预训练中使用的数据集不同，**它包含更多期望输出的目标数据**，这和互联网上的一些**偏好数据集**类似，也和一些聊天机器人类似，**他们在更具体的场景使用更加具体的数据集和语料**。

这些数据集和语言模型预训练使用的数据集相比在数量上要**少几个数量级**，因为它们更关注于**获得具体的内容**，就像一种真正**人性化交互式的文本类型**，而不是互联网上的大多数较为嘈杂并难以处理的数据。

![image-20230627232827874](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230627232827874.png)

经过从预定义数据集中的采样之后，语言模型会基于一个prompt将输出多个文本，之后我们会对其进行排名。这有点像我们将一个prompt输入到多个语言模型中，这些模型将生成不同的文本，然后我们**标记这些输出并进行相应的排名**。

![image-20230626214647187](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626214647187.png)

在得到模型的多个输出之后，我们可以采用一些排序方法例如Elo对它们进行逐个比较排序，有很多种排序方法都可以使用，但本质上都是使用一个较为人性化的方式将文本映射到下游的分数。

![image-20230626214806157](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626214806157.png)

在我们有了一些输出结果的排名之后，我们就需要考虑使用输入输出对来训练监督学习模型，具体的我们实际训练一系列的文本并将其作为输入，对其进行

![image-20230626214831505](https://evinci.oss-cn-hangzhou.aliyuncs.com/img/image-20230626214831505.png)
